# ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© ë¶€ë¶„ ì„¤ëª…

## ğŸ“ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‚¬ìš©ë˜ëŠ” ìœ„ì¹˜

### 1. TransformerEncoderLayer ë‚´ë¶€ (í•µì‹¬)

**íŒŒì¼**: `train_model.py`, `generate_text.py`

```python
# train_model.py ë¼ì¸ 67-74
encoder_layers = nn.TransformerEncoderLayer(
    d_model=d_model,           # 256
    nhead=nhead,               # 8 (Multi-Head Attentionì˜ í—¤ë“œ ìˆ˜)
    dim_feedforward=dim_feedforward,  # 1024
    dropout=dropout,
    batch_first=False
)
self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
```

**ì„¤ëª…**:
- `nn.TransformerEncoderLayer`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ **Multi-Head Self-Attention** ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•©ë‹ˆë‹¤
- `nhead=8`ì€ 8ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤
- ê° í—¤ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ì„ ê³„ì‚°í•˜ê³ , ê²°ê³¼ë¥¼ ê²°í•©í•©ë‹ˆë‹¤

### 2. Forward Passì—ì„œ ì–´í…ì…˜ í˜¸ì¶œ

**íŒŒì¼**: `train_model.py` ë¼ì¸ 89-101

```python
def forward(self, src, src_mask=None):
    # ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©
    src = self.embedding(src) * math.sqrt(self.d_model)
    src = self.pos_encoder(src)
    src = self.dropout(src)
    
    # Transformer ì¸ì½”ë” (ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì—¬ê¸°ì„œ ì‹¤í–‰ë¨)
    output = self.transformer(src, src_key_padding_mask=None, mask=src_mask)
    
    # ì¶œë ¥
    output = self.fc_out(output)
    return output
```

**ì„¤ëª…**:
- `self.transformer()` í˜¸ì¶œ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤
- `src_mask`ëŠ” Causal Attention Maskë¡œ, ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ì—†ë„ë¡ í•©ë‹ˆë‹¤

### 3. Causal Attention Mask ìƒì„±

**íŒŒì¼**: `train_model.py` ë¼ì¸ 103-107, `generate_text.py` ë¼ì¸ 60-63

```python
def create_causal_mask(seq_len, device):
    """Causal mask ìƒì„± (ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ì—†ë„ë¡)"""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    mask = mask.masked_fill(mask == 1, float('-inf'))
    return mask.to(device)
```

**ì„¤ëª…**:
- ì´ ë§ˆìŠ¤í¬ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ë„ë¡ í•©ë‹ˆë‹¤
- ìƒì‚¼ê° í–‰ë ¬(upper triangular matrix)ì„ ë§Œë“¤ì–´ì„œ ë¯¸ë˜ ìœ„ì¹˜ë¥¼ `-inf`ë¡œ ì„¤ì •
- ì–¸ì–´ ëª¨ë¸ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ë³´ì¥

### 4. í•™ìŠµ ë° ìƒì„± ì‹œ Mask ì‚¬ìš©

**íŒŒì¼**: `train_model.py` ë¼ì¸ 120-150

```python
def train_epoch(model, dataloader, optimizer, criterion, device, epoch, config):
    # Causal mask ìƒì„±
    causal_mask = create_causal_mask(config['max_seq_length'], device)
    
    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs = inputs.transpose(0, 1).to(device)
        targets = targets.transpose(0, 1).to(device)
        
        # Forward pass (ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰)
        outputs = model(inputs, src_mask=causal_mask)  # <-- ì—¬ê¸°ì„œ ì–´í…ì…˜ ì‹¤í–‰
```

**íŒŒì¼**: `generate_text.py` ë¼ì¸ 140-160

```python
def generate_text(...):
    # Causal mask ìƒì„±
    causal_mask = create_causal_mask(seq_len, device)
    
    # ëª¨ë¸ ì˜ˆì¸¡ (ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰)
    outputs = model(input_tensor, src_mask=causal_mask)  # <-- ì—¬ê¸°ì„œ ì–´í…ì…˜ ì‹¤í–‰
```

## ğŸ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‘ë™í•˜ëŠ” ë°©ì‹

### Multi-Head Self-Attention ë‚´ë¶€ ë™ì‘ (ê°œë…ì  ì„¤ëª…)

PyTorchì˜ `TransformerEncoderLayer` ë‚´ë¶€ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì–´í…ì…˜ ê³„ì‚°ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:

1. **Query, Key, Value ìƒì„±**:
   ```
   Q = src Ã— W_q  (Query)
   K = src Ã— W_k  (Key)
   V = src Ã— W_v  (Value)
   ```

2. **ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°**:
   ```
   Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
   ```

3. **Multi-Headë¡œ ë¶„í•  ë° ê²°í•©**:
   - 8ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ„ì–´ ê°ê° ì–´í…ì…˜ ê³„ì‚°
   - ê²°ê³¼ë¥¼ ê²°í•©(concatenate)í•˜ì—¬ ìµœì¢… ì¶œë ¥ ìƒì„±

4. **Causal Mask ì ìš©**:
   - ë§ˆìŠ¤í¬ë¥¼ í†µí•´ ë¯¸ë˜ í† í°ì˜ ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ `-inf`ë¡œ ì„¤ì •
   - softmax í›„ ë¯¸ë˜ í† í°ì˜ ê°€ì¤‘ì¹˜ëŠ” 0ì´ ë¨

## ğŸ“Š ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì„¤ì •

í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì–´í…ì…˜ ì„¤ì •:

- **ì–´í…ì…˜ í—¤ë“œ ìˆ˜**: 8ê°œ (`nhead=8`)
- **ì„ë² ë”© ì°¨ì›**: 256 (`d_model=256`)
- **ê° í—¤ë“œì˜ ì°¨ì›**: 256 / 8 = 32
- **ë ˆì´ì–´ ìˆ˜**: 4ê°œ (ê° ë ˆì´ì–´ë§ˆë‹¤ ì–´í…ì…˜ ì‹¤í–‰)
- **ë§ˆìŠ¤í¬ íƒ€ì…**: Causal Mask (ë¯¸ë˜ í† í° ì°¨ë‹¨)

## ğŸ’¡ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì—­í• 

1. **ë¬¸ë§¥ ì´í•´**: ê° í† í°ì´ ì‹œí€€ìŠ¤ ë‚´ ë‹¤ë¥¸ í† í°ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ë©€ë¦¬ ë–¨ì–´ì§„ í† í°ë“¤ ê°„ì˜ ê´€ê³„ë„ í¬ì°©
3. **ê°€ì¤‘ì¹˜ ê³„ì‚°**: ì–´ë–¤ í† í°ì— ë” ì§‘ì¤‘í• ì§€ ìë™ìœ¼ë¡œ í•™ìŠµ
4. **ë³‘ë ¬ ì²˜ë¦¬**: ëª¨ë“  í† í° ìŒì˜ ì–´í…ì…˜ì„ ë™ì‹œì— ê³„ì‚° ê°€ëŠ¥

## ğŸ”§ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•

ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ë ¤ë©´:

```python
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, nhead):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.size()
        
        # Query, Key, Value ìƒì„±
        Q = self.W_q(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # ë§ˆìŠ¤í¬ ì ìš©
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax ë° Valueì™€ ê³±í•˜ê¸°
        attn_weights = F.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, V)
        
        # í—¤ë“œ ê²°í•©
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.W_o(attn_output)
```

## ğŸ“ ìš”ì•½

**ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ìœ„ì¹˜:**

1. âœ… `nn.TransformerEncoderLayer` - Multi-Head Self-Attention ë‚´ì¥
2. âœ… `self.transformer()` í˜¸ì¶œ ì‹œ - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰
3. âœ… `create_causal_mask()` - Causal Attention Mask ìƒì„±
4. âœ… í•™ìŠµ ë° ìƒì„± ì‹œ `src_mask` íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬

**í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œëŠ” PyTorchì˜ ë‚´ì¥ êµ¬í˜„ì„ ì‚¬ìš©**í•˜ê³  ìˆì–´, ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

